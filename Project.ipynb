{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonalvrshny/IR23-MRRS/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdEkLO2CuDbW",
        "outputId": "23bf946d-7285-4d40-bff4-210f9d3d79ee"
      },
      "outputs": [],
      "source": [
        "%pip install nltk spacy tensorflow torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oiarHuyuPL9",
        "outputId": "29c7ba6a-858d-4b55-9b7d-fb30c8f6888e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: six in c:\\users\\sonal\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=96fef068856ce6231a54efb3661bef1508538f3ae67a74d8e8aa52877237de5c\n",
            "  Stored in directory: c:\\users\\sonal\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CoUt8iPYuH8S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "en\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect\n",
        "\n",
        "def detect_language(query):\n",
        "    try:\n",
        "        return detect(query)\n",
        "    except Exception as e:\n",
        "        return \"Error: \" + str(e)\n",
        "    \n",
        "print(detect_language(\"how to make burger\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um28Z3WLkfXv",
        "outputId": "5753eaf5-38bf-4d82-94f7-d153a60a19cb"
      },
      "outputs": [],
      "source": [
        "# For english\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# For spanish\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "# For hindi\n",
        "!python -m spacy download xx_ent_wiki_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gMtycYb2uN4l"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the language model for each language\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_es = spacy.load(\"es_core_news_sm\")\n",
        "nlp_hi = spacy.load(\"en_core_web_sm\")  # Hindi is not directly supported, so using a multilingual model\n",
        "\n",
        "def parse_query(query, lang):\n",
        "    if lang == 'en':\n",
        "        doc = nlp_en(query)\n",
        "    elif lang == 'es':\n",
        "        doc = nlp_es(query)\n",
        "    elif lang == 'hi':\n",
        "        doc = nlp_hi(query)\n",
        "    else:\n",
        "        return \"Unsupported language\"\n",
        "\n",
        "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "    return keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rwa27hNGuYRl"
      },
      "outputs": [],
      "source": [
        "def process_query(query):\n",
        "    lang = detect_language(query)\n",
        "    if \"Error\" in lang:\n",
        "        return lang\n",
        "    keywords = parse_query(query, lang)\n",
        "    return {\"language\": lang, \"keywords\": keywords}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'recipeName': 'मार्गेरीता पिज़्ज़ा', 'ingredients': ['2 कप मैदा (flour)', '1 कप पानी (water)', '1 छोटा चम्मच खमीर (1 tsp yeast)', '1 छोटा चम्मच नमक (1 tsp salt)', '1 बड़ा चम्मच जैतून का तेल (1 tbsp olive oil)', '1/2 कप टमाटर की सॉस (1/2 cup tomato sauce)', '1 कप मोज़ारेला चीज़ (1 cup mozzarella cheese)', 'ताजगी वाले तुलसी पत्तियां (Fresh basil leaves)', 'नमक और काली मिर्च स्वाद के अनुसार (Salt and pepper to taste)'], 'instruction': ['पानी में खमीर घोलकर मैदा, नमक, और तेल के साथ मिश्रित डो को बनाएं। 1 घंटे के लिए राइज़ होने दें।', 'ओवन को 475°F (245°C) पर प्रीहीट करें।', 'डो को बेलन से बेलें और टमाटर की सॉस लगाएं।', 'मोज़ारेला चीज़ और बेसिल पत्तियां डालें।', 'नमक और काली मिर्च से सीज़न करें।', 'क्रस्ट सुनहरा होने तक 10-15 मिनट तक बेक करें।']}\n"
          ]
        }
      ],
      "source": [
        "# Load recipe JSON data into a list of dictionaries\n",
        "import json\n",
        "with open('recipes.json', 'r') as file:\n",
        "    recipes = json.load(file)\n",
        "\n",
        "print(recipes[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nhDGVuCu2Va",
        "outputId": "59791af9-c5f9-460b-95cf-8b2add2cb616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How to make traditional Mexican guacamole?\n",
            "Result: {'language': 'en', 'keywords': ['traditional', 'mexican', 'guacamole']}\n",
            "\n",
            "Query: Receta de paella de mariscos\n",
            "Result: {'language': 'es', 'keywords': ['Receta', 'paella', 'marisco']}\n",
            "\n",
            "Query: बटर चिकन की रेसिपी\n",
            "Result: {'language': 'hi', 'keywords': ['बटर']}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_queries = [\"How to make traditional Mexican guacamole?\", \"Receta de paella de mariscos\", \"बटर चिकन की रेसिपी\"]\n",
        "\n",
        "for query in sample_queries:\n",
        "    result = process_query(query)\n",
        "    print(f\"Query: {query}\\nResult: {result}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "     ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
            "     ------- -------------------------------- 10.2/55.1 kB ? eta -:--:--\n",
            "     --------------------- ---------------- 30.7/55.1 kB 435.7 kB/s eta 0:00:01\n",
            "     -------------------------------------- 55.1/55.1 kB 477.2 kB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi in c:\\users\\sonal\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.11.17)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.0/1.5 MB 1.9 MB/s eta 0:00:01\n",
            "     ----- ---------------------------------- 0.2/1.5 MB 2.1 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 0.3/1.5 MB 2.4 MB/s eta 0:00:01\n",
            "     ------------ --------------------------- 0.5/1.5 MB 2.6 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 0.6/1.5 MB 2.6 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 0.9/1.5 MB 3.1 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 1.1/1.5 MB 3.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 1.4/1.5 MB 3.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  1.5/1.5 MB 3.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 1.5/1.5 MB 3.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: sniffio in c:\\users\\sonal\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "     ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
            "     -------------------------------------- 133.4/133.4 kB 3.8 MB/s eta 0:00:00\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
            "     ---------------------------------------- 58.8/58.8 kB 3.2 MB/s eta 0:00:00\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "     ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "     ---------------------------------------- 0.0/53.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 53.6/53.6 kB 2.7 MB/s eta 0:00:00\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "     ---------------------------------------- 0.0/65.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 65.0/65.0 kB 3.4 MB/s eta 0:00:00\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py): started\n",
            "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17461 sha256=3a107944c0b70dd16fde6a8baa7ebf7d2053585b84097c1de2b4bc153a9d1874\n",
            "  Stored in directory: c:\\users\\sonal\\appdata\\local\\pip\\cache\\wheels\\39\\17\\6f\\66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "npKyMLdMvFwZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hindi': 'पारंपरिक मैक्सिकन गुआकामोल कैसे बनाएं?', 'spanish': '¿Cómo hacer guacamole mexicano tradicional?', 'english': 'How to make traditional Mexican guacamole?'}\n",
            "{'hindi': 'सीफूड पेला नुस्खा', 'spanish': 'Receta de paella de mariscos', 'english': 'Seafood Paella Recipe'}\n",
            "{'hindi': 'बटर चिकन की रेसिपी', 'spanish': 'Receta de pollo con mantequilla', 'english': 'Butter chicken recipe'}\n"
          ]
        }
      ],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "def translate_query(query, target_language):\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(query, dest=target_language)\n",
        "    return translation.text\n",
        "\n",
        "def get_translated_queries(user_query):\n",
        "    lang = detect_language(user_query)\n",
        "    if lang == \"hi\":\n",
        "        translated_queries = {\n",
        "                'hindi': user_query,\n",
        "                'spanish': translate_query(user_query, 'es'),\n",
        "                'english': translate_query(user_query, 'en')\n",
        "            }\n",
        "    elif lang == \"es\":\n",
        "        translated_queries = {\n",
        "                'hindi': translate_query(user_query, 'hi'),\n",
        "                'spanish': user_query,\n",
        "                'english': translate_query(user_query, 'en')\n",
        "            }\n",
        "    elif lang == \"en\":\n",
        "        translated_queries = {\n",
        "                'hindi': translate_query(user_query, 'hi'),\n",
        "                'spanish': translate_query(user_query, 'es'),\n",
        "                'english': user_query\n",
        "            }\n",
        "    return translated_queries\n",
        "\n",
        "user_queries = [\"How to make traditional Mexican guacamole?\", \"Receta de paella de mariscos\", \"बटर चिकन की रेसिपी\"]\n",
        "for q in user_queries:\n",
        "    print(get_translated_queries(q))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "def embed_text(text, model):\n",
        "    return model.encode(text)\n",
        "\n",
        "def build_index(database_embeddings):\n",
        "    dim = len(database_embeddings[0])\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(np.array(database_embeddings).astype('float32'))\n",
        "    return index\n",
        "\n",
        "def search_database(query_embedding, index, database, k=2):\n",
        "    _, indices = index.search(np.array([query_embedding]).astype('float32'), k)\n",
        "    # Return sorted list of recipes and their similarity scores\n",
        "    return [{'recipe': database[i]['recipeName'], 'similarity': 1.0} for i in indices[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chicken curry\n",
            "{'hindi': 'चिकन करी', 'spanish': 'curry de pollo', 'english': 'chicken curry'}\n",
            "{'hindi': [{'recipe': 'चिकन करी', 'similarity': 1.0}, {'recipe': 'पनीर टिक्का', 'similarity': 1.0}], 'spanish': [{'recipe': 'Pollo al Curry', 'similarity': 1.0}, {'recipe': 'Chicken Curry', 'similarity': 1.0}], 'english': [{'recipe': 'Chicken Curry', 'similarity': 1.0}, {'recipe': 'Pollo al Curry', 'similarity': 1.0}]}\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained model for sentence embeddings\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Extract recipe titles\n",
        "recipe_titles = [item['recipeName'] for item in recipes]\n",
        "\n",
        "# Embed all recipe texts\n",
        "database_embeddings = [embed_text(text, model) for text in recipe_titles]\n",
        "\n",
        "# Build FAISS index\n",
        "index = build_index(database_embeddings)\n",
        "\n",
        "user_query = \"how to make chicken curry\"\n",
        "processed_query = process_query(user_query)\n",
        "print(\" \".join(processed_query['keywords']))\n",
        "translated_queries = get_translated_queries(\" \".join(processed_query['keywords']))\n",
        "print(translated_queries)\n",
        "# Embed the translated queries\n",
        "query_embeddings = {lang: embed_text(translated_query, model) for lang, translated_query in translated_queries.items()}\n",
        "\n",
        "# Search the database for each translated query using FAISS\n",
        "results = {}\n",
        "for lang, query_embedding in query_embeddings.items():\n",
        "    results[lang] = search_database(query_embedding, index, recipes)\n",
        "\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOwV8CSw47rroy6rURZjagi",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
